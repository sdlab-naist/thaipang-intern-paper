% !TEX root = ../paper.tex

\section{Case Study Results}
\pick{I'm exhausted...sorry. My writing here is pretty bad, hope you understand what I'm trying to say :)}
Table \ref{tb:datastatistic} summarizes data set we used for this study after preparation.
From 72,484 comments of 6,605 reviews with its commit messages extracted from raw data set, there are 10,670 (15\%) of comments are written by reviewers while 85\% of comments are automatically generated by Gerrit system and integration bots.  

\begin{table}[!h]
\caption{A summary data sets and some statistics.}
\centering
\small
\begin{tabular}{ccc}
\hline
& Total Number & Percentage \\ \hline \hline
Commit Messages & 6,605 &  -  \\ \hline
All Comments & 72,484& - \\ \hline
Reviewers Messages & 10,670 & 15\% \\ \hline
Automated Messages. & 61,814 & 85\% \\ \hline 

\end{tabular}
\label{tb:datastatistic}
\end{table}



%\pick{I realize that your figure is better for the presentation :)}
%\begin{figure}[h]
%\centering
%\includegraphics[width=3.2in]{filter}
%\caption{The filtering process.}
%\label{fig:filter}
%\end{figure}
%\subsection{Data Preparation}
%\subsubsection{Data extraction}
%
%6,605 changes and 72,484 comments have been extracted from the raw dataset.
%35\% of the comments are by the system or one of the bots, and are thus discarded.
%
%\subsubsection{Common pattern removal}
%
%By looking for most frequent lines, common patterns were found, such as \emph{`Uploaded patch set 2.'} and \emph{`Change has been successfully cherry-picked to the staging branch as \dots'}.
%
%Removing occurrences of these patterns leaves 77\% of the remaining comments empty.
%This probably means these comments solely contain automatically-generated text, and are thus discarded.
%This leaves us with 10,670 comments and 6,605 commit messages; a total of 17,275 documents.
%
%\subsubsection{Tokenizing}
%
%After the tokenization step, 393,238 tokens are generated in total. They are composed of 20,025 different words.
%This means that each document will be converted into a vector of 20,025 dimensions, each dimension representing a single word.
%


\noindent \textbf{RQ1: Can we identify useful and useless discussions in code review?}

From 320 samples, the number of labeled comments were 87, 60, 51, and 122 for 0, 1, 2, and 3 \texttt{YES} votes, respectively.
To estimated estimated similarity and dissimilarity thresholds, we iterated $s_t$, $d_t$, $s'_t$, and $d'_t$ values from \TODO{.......}. \pick{We cannot brute force for every floating number. How did we calculate this?} Table \ref{tb:thresholds} shows 5 sets of thresholds that best classify useful and useless comments based on F-measure score. \pick{I used dummy results. Please change it} \pick{What we got from this table? What information can we interpret? The threshold is slightly different? Is it good or bad? Please describes.} 

\textbf{Model Effectiveness.} Figure \ref{fig:scatter} shows relationship between comments class and our similarity thresholds. The green area represents useful model $\Theta(c,S_T=0.09028129266684175,D_T= 2.245146382888072)$ and the red area represents useless classification model $\Omega(c,S_T=0.09028129266684175,D_T= 2.245146382888072)$. This figure also shows an example comment that fall in the different area\pick{Didn't put yet}. As shown in the figure, most of comments fall in green area is useful comments and very few useless comments fall in this area. Similarly, most of comments fall in red area is useless comments. However, some comments were not determined (in white area). From the example comment, it shows that these comments discussed on ..... which did not contains any similar words. Moreover, the areas of these two models are overlap. Table \ref{tb:classify_number} summarizes the number of classification from this results. Interestingly, 87\% useless comments classified by our model have \texttt{YES} = 0 and 1 voting scores. This is same as the results of useful class that most of them have \texttt{YES} = 2 and 3 voting scores
This finding suggests that these unclear comments have something.....

\begin{table}[h]
\centering
\small
\caption{Number of comments classified by our approach against ground truth data }
\begin{tabular}{ccccc}
\hline
& \multicolumn{4}{c}{Class from Ground Truth Data} \\ \cline{2-5}
Our&  Useless  & Unclear  & Unclear & Useful \\
Classifications&  (\texttt{YES}=0) & (\texttt{YES}=1) & (\texttt{YES}=2) & (\texttt{YES}=3) \\
\hline \hline
Useless & 69 & 23 & 7 & 6 \\
Useful & 4 & 13 & 24 & 95 \\
Overlap & 1 & 1 & 0 & 1 \\
Undetermined & 12 & 24 & 20 & 20 \\
\hline
\end{tabular}
\label{tb:classify_number}
\end{table}


\begin{table*}[!t]
\caption{An accuracy of similarity and dissimilarity thresholds for useful and useless comment classifications}
\small
\centering
\def\arraystretch{1.2}
\begin{tabular}{ccccccc}
\hline
Prediction Models  & Rank & $s_t$ & $d_t$ & F-measure & Precision & Recall \\ \hline \hline
\multirow{7}{*}{$\Theta(c,S_T=s_t,D_T=d_t)$} & 1 & 0.09028129266684175 & 2.245146382888072 &  1.00000 & 1.00000 & 1.00000 \\ \cline{2-7}
& 2 & 0.09028129266684175 & 2.245146382888072 &  1.00 & 1.00 & 1.00 \\ \cline{2-7}
& 3 & 0.09028129266684175 & 2.245146382888072 &  1.00 & 1.00& 1.00 \\ \cline{2-7}
& 4 & 0.09028129266684175 & 2.245146382888072 &  1.00 & 1.00 & 1.00 \\ \cline{2-7}
& 5 & 0.09028129266684175 & 2.245146382888072 &  1.00 & 1.00 & 1.00 \\ \cline{2-7}
& \multicolumn{3}{r}{Average} &  1.00 & 1.00 & 1.00 \\ \cline{2-7}
& \multicolumn{3}{r}{Min-Max} &   1.00 - 1.00 & 1.00 - 1.00  & 1.00 - 1.00  \\ \hline \hline
\multirow{7}{*}{$\Omega(c,S_T=s_t,D_T=d_t)$} & 1 & 0.09028129266684175 & 2.245146382888072 &  1.00000 & 1.00000 & 1.00000 \\ \cline{2-7}
& 2 & 0.09028129266684175 & 2.245146382888072 &  1.00 & 1.00 & 1.00 \\ \cline{2-7}
& 3 & 0.09028129266684175 & 2.245146382888072 &  1.00 & 1.00& 1.00 \\ \cline{2-7}
& 4 & 0.09028129266684175 & 2.245146382888072 &  1.00 & 1.00 & 1.00 \\ \cline{2-7}
& 5 & 0.09028129266684175 & 2.245146382888072 &  1.00 & 1.00 & 1.00 \\ \cline{2-7}
& \multicolumn{3}{r}{Average} &  1.00 & 1.00 & 1.00 \\ \cline{2-7}
& \multicolumn{3}{r}{Min-Max} &   1.00 - 1.00 & 1.00 - 1.00  & 1.00 - 1.00  \\ \hline
\end{tabular}
\label{tb:thresholds}
\end{table*}

\textbf{Model Robustness.} After we performed cross validation, we obtained the following result:
for useful and useless classifications,
we obtained an average F$_1$ score of 0.693 and 0.681,
with standard deviation of 0.090 and 0.118, respectively.
This indicates that we can use our approach to classify with confident of 69.8\%.
\pick{Precision? Recall?}

According to the results, we can answer  RQ1 that \textit{we can automatically classify usefulness of comments from similarity and dissimilarity using our approach.}

%After we performed cross validation, we obtained the following result:
%for positive and negative classifications,
%we obtained an average F$_1$ score of 0.693 and 0.681,
%with standard deviation of 0.090 and 0.118, respectively.
%This indicates that we can use our approach to classify with confident of 69.8\%.

%After the similarity and distance metrics have been calculated,
%these metrics appear to be able to separate the useful comments from the non-useful ones,
%as can be seen in Fig.\ref{fig:scatter}.
%Note that many comments have a cosine similarity metric of 0.
%This is because the comment text and the corresponding commit message has no word in comment.

\begin{figure}[!t]
\centering
\includegraphics[scale=0.45, trim=0 0 30 50, clip=true]{scatter_log}
\caption{The similarity and distance plot of the training data.
The symbol represents the score, which ranges from 0 to 3.}
\label{fig:scatter}
\end{figure}

\noindent \textbf{RQ2: Do code reviewers intensively discuss on the proposed changes?}\\
\pick{The questions: How many \% automatic / useful/ useless/ unclear messages for each reviews? Does these numbers have a relationship with factor of code review (e.g. reviewing time)?}

\pick{If there is high \% of useless messages, it means ineffective review, isn't it?}

\noindent \textbf{RQ3: Does this approach practical and cost-effective for large-scale projects?}\\


%\subsection{Model Generation}

%% the criteria that were found
%From our training data,
%the following criteria have been obtained that maximizes the F$_1$ score:
%
%Criteria to find samples with score of 3 (``positive''):
%\begin{gather*} similarity \geq 0.015528 \text{ and } distance \leq 2.494944
%\\ \text{(F$_1$-score: 0.741313)} \end{gather*}
%
%Criteria to find samples with score of 0 (``negative''):
%\begin{gather*} similarity \leq 0.087522 \text{ and } distance \geq 2.265679
%\\ \text{(F$_1$-score: 0.725389)}\end{gather*}
%
%Note that these constant values can vary from project to project, and thus is not a universal constant.
%
%% run result
%
%Running our model against the training data gives us the result displayed in the following table.
%In the table, \emph{Neither} means that the comment did not meet either criteria, while \emph{Overlap} means that the comment  met both ``positive'' and ``negative'' criteria.
%
%\begin{center}
%\begin{tabular}{|r|rrrr|}
%\hline
%& \bfseries 0 & \bfseries 1 & \bfseries 2 & \bfseries 3 \\
%\hline
%Negative & 69 & 23 & 7 & 6 \\
%Neither & 12 & 24 & 20 & 20 \\
%Overlap & 1 & 1 & 0 & 1 \\
%Positive & 4 & 13 & 24 & 95 \\
%\hline
%\end{tabular}
%\end{center}
%
%As expected, most comments classified as \emph{negative} have score of 0 and 1,
%while most \emph{positive} comments have score of 2 and 3.
%However, some comments in every score type were classified as \emph{neither}.
%
%



% cross validation result