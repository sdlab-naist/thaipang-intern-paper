% !TEX root = ../paper.tex

\section{Case Study}

Our case study uses the Qt project review history from the Gerrit code review tool provided by Hamasaki et. al. \cite{Hamasaki2013}. Qt project is a large open source project composed of numerous small subsystems. We selected the most active subproject, which is called \texttt{qtbase}.
From this subproject, we used the review history from May 2011 to June 2012, which contains 6,605 changes and 72,484 comments.


\subsection{Data Preparation}
We use the commit messages and comments within Gerrit as our data set.
Before classifying the usefulness of comments, we first processed the commit message and comments as follows: 

\subsubsection{Removal of automatically generated messages} To consider only reviewers discussion, we ignore all messages that were automatically generated e.e. bot messages.
%In Gerrit, the comment history is composed of human-written comments from reviewers and automatically generated messages.
These automatic messages are generated by Gerrit, the continuous integration system, and the sanity bot\footnote{It is a bot that automatically checks new proposed changes for trivial sanity issues, such as line endings, copyright notices, and commit messages} to record activities. \textit{`Upload patch set 1.'}, \textit{`Change has been successfully cherry-picked to the staging branch as ...'}, and \textit{`Sanity review passed'} are some examples. In practice, they are not considered in a code review  and are not substantively relevant to the proposed changes and do not directly impact software quality\cite{Mcintosh}. 

We first find and remove all comments written by bots (i.e. \emph{Qt Continuous Integration System} and \emph{Qt Sanity Bot}). These are easily detected by their automatically generated signature 
e.g. XXX 
\dan{good example here} 
Next, we removed messages, including reviewers comments, with common useless patterns or occur frequently using regular expressions. Some example are XXX
\dan{good example here} 
This leaves us only human-written comments that cannot automatically be classified as useless.

%, . and hence are useless by our definition.
%%They are easily determined as useless by our method and would artificially improve our classification performance.
%These messages are identified by looking for the most common lines of text using regular expression.
%%Regular expression patterns are then constructed to match and remove these messages.
%Occurrences of these patterns are then removed from our dataset prior to further preprocessing.
%
%Automatic messages are also often inserted as part of review comments.
%They too are removed from our dataset, leaving us with only human-written part.
%We also completely ignored comments whose author is \emph{Qt Continuous Integration System} and \emph{Qt Sanity Bot}.

\subsubsection{Data preprocessing}
As is customary for VSM processing, we extracted semantic words from commit messages and comment messages before converting to vector.
For each message, we removed all punctuation signs (except apostrophe) and other non alphanumeric characters. We also removed common words (e.g. a, an, the) using Google stop word list\footnote{Available at \url{http://meta.wikimedia.org/wiki/Stop_word_list/google_stop_word_list#English}}. We then used Porter stemming algorithm to remove the commoner morphological and inflexional endings from words in English.

Table \ref{tb:datastatistic} summarizes data set we used for this study after preparation. 

\begin{table}[!h]
\caption{A summary data sets and some statistics.}
\centering
\small
\begin{tabular}{ccc}
\hline
& Total Number & Percentage \\ \hline \hline
Commit Messages & 6,605 &  -  \\ \hline
All Comments & 72,484& - \\ \hline
Reviewers Comments & 10,583 & 15\% \\ \hline
Automated Comments & 61,814 & 85\% \\ \hline 

\end{tabular}
\label{tb:datastatistic}
\end{table}

\subsection{Manual Comment Usefulness Assessment}
In this case study, three participants (the first two authors and one student) independently assessed comment usefulness by addressing the question, ``Is this comment directly useful for approving or modifying the change request?''
Then, the participants gave a vote for \texttt{YES} if the comment is likely to be useful and \texttt{NO} otherwise.
From the voting scores, we regarded comments with three \texttt{YES} votes as \emph{useful} comments, and comments with no \texttt{YES} votes (i.e. three \texttt{NO} votes) are \emph{useless} comments. For the comments with one and two \texttt{YES} votes, we defined them to have \emph{unclear} usefulness. 

\subsection{Research Questions}
% We addressed two research questions: \textbf{RQ1:} Is semantic similarity a good indicator of MCR comment usefulness? and \textbf{RQ2:} Is semantic similarity classification cost-efficient, assurable, and scalable?.
% \pick{Do we need motivation for these RQs?}
% Thai sez: these recitation of research questions are very redundant...
%           for the motivation, I think it should have been discussed at the introduction.

%\pick{I realize that your figure is better for the presentation :)}
%\begin{figure}[h]
%\centering
%\includegraphics[width=3.2in]{filter}
%\caption{The filtering process.}
%\label{fig:filter}
%\end{figure}
%\subsection{Data Preparation}
%\subsubsection{Data extraction}
%
%6,605 changes and 72,484 comments have been extracted from the raw dataset.
%35\% of the comments are by the system or one of the bots, and are thus discarded.
%
%\subsubsection{Common pattern removal}
%
%By looking for most frequent lines, common patterns were found, such as \emph{`Uploaded patch set 2.'} and \emph{`Change has been successfully cherry-picked to the staging branch as \dots'}.
%
%Removing occurrences of these patterns leaves 77\% of the remaining comments empty.
%This probably means these comments solely contain automatically-generated text, and are thus discarded.
%This leaves us with 10,670 comments and 6,605 commit messages; a total of 17,275 documents.
%
%\subsubsection{Tokenizing}
%
%After the tokenization step, 393,238 tokens are generated in total. They are composed of 20,025 different words.
%This means that each document will be converted into a vector of 20,025 dimensions, each dimension representing a single word.


\begin{ResearchQuestions}
\item[RQ1:] Is semantic similarity a good indicator of MCR comment usefulness?
\end{ResearchQuestions}

\begin{figure}[!t]
\centering
\includegraphics[scale=0.45, trim=0 0 30 50, clip=true]{scatter_log}
\caption{The similarity and distance plot of the training data.
The symbol represents the score, which ranges from 0 to 3.
The green area represents the \emph{useful} classification model and the red area represents the \emph{useless} model.}
\label{fig:scatter}
\end{figure}

To answer this question, we randomly sampled 318 comments from our data set and manually assessd their usefulness.
Then, we used these comments for both training and test data for our approach to determine its effectiveness.
We also determined the confidence in the effectiveness estimates using bootstrapping cross validation to estimate variability in the performance measures due to using sample data.

% \dan{What what the effort in person-hours for this?}
% Thai sez:  Average time per comment is 28 seconds.
%            Since we do a lot of other things while training, it gives us a STDEV of 45 seconds.
%            The median, by the way, is just 10 seconds.

% Since our models do not include unclear type of comments, we defined them as negative condition i.e useless comments in case of useful classification (when using $\Theta(c,S_T,D_T)$ model) and useful comments in case of useless classification (when using $\Omega(c,S'_T,D'_T)$ model).
%\dan{I thought we changed this! It would be bad if we didn't. Let me know what the actual situation is here.}
%
% Thai sez:  We did not. Maybe this is just a matter of wording. I believe what the text says is:
%                - For useful classification, we only look for 3 scored comments.
%                - For useless classification, we only look for 0 scored comments.

To determine similarity and dissimilarity thresholds for our model, we iterated $S_t$ and $D_t$ values from every unique similarity and dissimilarity value in our data set.
%Table \ref{tb:thresholds} shows 5 sets of thresholds that best classify useful and useless comments based on F$_1$ score.
We selected the thresholds that best classify useful and useless comments based on maximizing the F1-measure score.
The models are $\Theta(c,S_T=0.015529,D_T=2.494944)$ for useful comments, and $\Omega(c,S'_T=0.087522,D'_T=2.265679)$ for useless comments.


\textbf{Model Effectiveness Results:} From 318 samples, the comment sets from manual assessment were 85 were useless (\texttt{YES} = 0),  111 were unclear (\texttt{YES} = 1 or 2), and 122 were useful (\texttt{YES} = 3). The precision and recall were 0.701 and 0.787 for useful model, and 0.648 and 0.824 for useless model. 

Figure \ref{fig:scatter} shows the relationship between usefulness classes from our model and comment sets from manual assessment. 
The green area represents the \emph{useful} classification model and the red area represents the \emph{useless} classification model. The comments drawn in the green area are those that satisfied the condition for useful comments, and the comments drawn in the red area are those satisfied the useless condition, while the comments drawn in the white area are those not satisfied any condition (i.e. \emph{undetermined}) and hence remain unclassified. 
The total number of comments from this classification results is described in Table \ref{tb:classify_number}. 



%%Interpret: comments & model%%%%
Corresponding to the precision and recall values, the figure shows that the majority of comments drawn in green area are comments from the useful set, and very few comments from useless set fall in this area.
Similarly, the majority of comments drawn in the red area are comments from the useless set.
It also shows that the similarity and dissimilarity values of comments in each set are not different from other comments in the same set.
Most comments from the useful set (\texttt{YES} = 3) are in the top left of the graph
while those from the useless set (\texttt{YES} = 0) adhere in the bottom right of the graph.
However, the unclear set still spread out over the similarity intervals and therefore we cannot determine 
a relationship with their similarity values. That is, we have no evidence to support that the undetermined set corresponds unclear comments and hence undetermined is not a useful classification.

There is an overlap section shown in Fig. \ref{fig:scatter}.
This is a conflict between the two classification conditions where a comment satisfies both simultaneously.  
This phenomenon can occur since similarity and dissimilarity threshold values of each group are ``fuzzy" and both models tend to maximize coverage of classification. Also similarity and dissimilarity are discreet and subject to some degree of granularity issues 
However, from the results in Table \ref{tb:classify_number}, only three comments were selected by both models (overlap).
In more extended experiments, less than 2\% were in the overlap set and this contradictory classification is relatively small.
Excluding the overlap classification results, we can calculate precision and recall as described in Table \ref{tb:classify_number}. Furthermore, there are 12 useless comments (14\%) and 21 useful comments (17\%) still are undetermined (drawn in white area). These comments were not satisfied our model suggesting that there are likely other indicators different from similarity that determine usefulness.

%From the example comment, it shows that these comments discussed on \TODO{insert topic}, but did not contain any word in common.

\begin{table}[!t]
\centering

\caption{Number of comments classified by our approach against ground truth data}
\begin{tabular}{cccccc}
\hline
Result from & \multicolumn{4}{c}{Results from manual assessment}  & \multirow{3}{*}{Precision}\\ \cline{2-5}
Classification &  Useless  & Unclear  & Unclear & Useful \\
Models&  (\texttt{YES}=0) & (\texttt{YES}=1) & (\texttt{YES}=2) & (\texttt{YES}=3) \\
\hline \hline
Useful Class & 3 & 12 & 24 & 95 & 0.709\\
Useless Class & 69 & 23 & 7 & 6 &  0.657\\
Overlap & 1 & 1 & 0 & 1 & - \\
Undetermined & 12 & 24 & 20 & 20 & - \\
\hline
Recall & 0.812 & - & - & 0.778\\ \hline 
\end{tabular}
\label{tb:classify_number}
\end{table}


%\begin{table*}[!t]
%\caption{An accuracy of similarity and dissimilarity thresholds for useful and useless comment classifications}
%\small
%\centering
%\def\arraystretch{1.2}
%\begin{tabular}{ccccccc}
%\hline
%Prediction Models  & Rank & $s_t$ & $d_t$ & F-measure & Precision & Recall \\ \hline \hline
%\multirow{5}{*}{\textbf{useful}: $\Theta(c,S_T=s_t,D_T=d_t)$}
%& 1 & 0.015529 & 2.494944 & 0.741 & 0.701 & 0.787 \\ \cline{2-7}
%& 2 & 0.015529 & 3.077129 & 0.740 & 0.693 & 0.795 \\ \cline{2-7}
%& 3 & 0.016713 & 2.494944 & 0.739 & 0.704 & 0.779 \\ \cline{2-7}
%& 4 & 0.015411 & 2.494944 & 0.738 & 0.696 & 0.787 \\ \cline{2-7}
%& 5 & 0.015411 & 3.077129 & 0.738 & 0.688 & 0.795
%% \\ \cline{2-7}
%% & \multicolumn{3}{r}{Average} &  1.00 & 1.00 & 1.00 \\ \cline{2-7}
%%& \multicolumn{3}{r}{Min-Max} &   1.00 - 1.00 & 1.00 - 1.00  & 1.00 - 1.00
%\\ \hline \hline
%\multirow{5}{*}{\textbf{useless}: $\Omega(c,S'_T=s_t,D'_T=d_t)$}
%& 1 & 0.087522 & 2.265679 & 0.725 & 0.648 & 0.824 \\ \cline{2-7}
%& 2 & 0.087522 & 2.250422 & 0.722 & 0.642 & 0.824 \\ \cline{2-7}
%& 3 & 0.087522 & 2.324771 & 0.720 & 0.663 & 0.788 \\ \cline{2-7}
%& 4 & 0.052856 & 2.265679 & 0.719 & 0.645 & 0.812 \\ \cline{2-7}
%& 5 & 0.087522 & 2.249675 & 0.718 & 0.636 & 0.824
%% \\ \cline{2-7}
%%& \multicolumn{3}{r}{Average} &  1.00 & 1.00 & 1.00 \\ \cline{2-7}
%%& \multicolumn{3}{r}{Min-Max} &   1.00 - 1.00 & 1.00 - 1.00  & 1.00 - 1.00
%\\ \hline
%\end{tabular}
%\label{tb:thresholds}
%\end{table*}

\begin{table*}[!t]
\caption{Results from bootstrapping cross validation of our classification models against random models}
\small
\centering
\def\arraystretch{1.2}
\begin{tabular}{cccc|cc|cc|cc}
\hline
\multicolumn{2}{c}{Classifcation}   & \multicolumn{2}{c|}{Precision} & \multicolumn{2}{c|}{Recall} & \multicolumn{2}{c|}{F-measure} & \multicolumn{2}{c}{Accuracy} \\ \cline{3-10}
\multicolumn{2}{c}{Models} & Avg. & STD. & Avg. & STD. & Avg. & STD. & Avg. & STD. \\ \hline \hline
\multirow{2}{*}{Useful} & $\Theta(c,S_T,D_T)$    &  0.654 & 0.116 &  0.759 & 0.123 & 0.693 & 0.089 & 0.752 & 0.067 \\ \cline{2-10}
& Random     &  0.421 & 0.114 &  0.376 & 0.116 & 0.496 & 0.144 & 0.496 & 0.089 \\ \hline
\multirow{2}{*}{Useless}  & $\Omega(c,S'_T,D'_T)$  &  0.636 & 0.144 &  0.755 & 0.148 & 0.681 & 0.118 & 0.815 & 0.064 \\ \cline{2-10}
& Random    &  0.336 & 0.131 &  0.269 & 0.115 & 0.478 & 0.182 & 0.500 & 0.089 \\
\hline
\end{tabular}
\label{tb:xvalidate}
\end{table*}


\textbf{Validation:}
We validated our approach using bootstrapping cross validation. By validation be mean to estimate the variability in the estimate of the 
performance measures from the training set to indicate how a accurate these are for the method in general.
We randomly selected 90\% of 318 comments for training set and determine thresholds.
The constructed model is then applied on the remaining 10\% comments as the validation set.
The precision, recall and F-measure scores are measured and recorded.
This validation was repeated 300 times to give the average and standard deviation for these performance measures. Bootstrapping is 
used because of concern about our relatively small sample size and possibly instability in using customary Jackknifing cross-validation.
For a baseline comparison, we also determined the performance of a random classifier (i.e. places items in classes with equal probability) using the same methods.

Table \ref{tb:xvalidate} describes the performance of useful and useless classification models including an average and standard deviation of precision, recall, and F-measure.
The results show that both of our models can achieve 60\% of precision and 75\% of recall, approximately.
In addition, while we do not present hypothesis test results, it is clear that our model significantly outperforms the random model on all performance measures e.g. mean Precision for Useless is notably higher than Random with about the same standard deviation so it's clearly significant. 


%\begin{table*}[!t]
%\caption{An accuracy of similarity and dissimilarity thresholds for useful and useless comment classifications}
%\small
%\centering
%\def\arraystretch{1.2}
%\begin{tabular}{ccccccc}
%\hline
%Prediction Models & $s_t$ & $d_t$ & F-measure & Precision & Recall \\ \hline \hline
%$\Theta(c,S_T=s_t,D_T=d_t)$   & 0.015529 & 2.494944 & 0.741 & 0.701 & 0.787 \\ \hline
%$\Omega(c,S'_T=s_t,D'_T=d_t)$ & 0.087522 & 2.265679 & 0.725 & 0.648 & 0.824 \\ \hline
%\end{tabular}
%\label{tb:thresholds}
%\end{table*}




According to our results, we can answer RQ1 as follows: \emph{using semantic similarity values as indicators, our approach can successfully classify comment usefulness with precision of 0.64 and recall of 0.75 approximately, which is significantly better than a random classifier.}

%After we performed cross validation, we obtained the following result:
%for positive and negative classifications,
%we obtained an average F$_1$ score of 0.693 and 0.681,
%with standard deviation of 0.090 and 0.118, respectively.
%This indicates that we can use our approach to classify with confident of 69.8\%.

%After the similarity and distance metrics have been calculated,
%these metrics appear to be able to separate the useful comments from the non-useful ones,
%as can be seen in Fig.\ref{fig:scatter}.
%Note that many comments have a cosine similarity metric of 0.
%This is because the comment text and the corresponding commit message has no word in comment.






\begin{ResearchQuestions}
\item[RQ2:] Is semantic similarity classification cost-efficient, assurable, and scalable?
\end{ResearchQuestions}

Automatic systems are generally known for saving human effort and also reduce errors.
We analyze the results to determine the performance of our models.
In addition, we also consider scalability of our models as it only validated for small numbers of comments. The concern is that the model may behave poorly at larger scale possibly leaving more manual assessment effort than is saved through using the method. 
% \pick{Trying to explain why we considering this}

\textbf{Cost Efficiency:}
% briefly explain
We determined cost efficiency in terms of time use for manual assessment.
For 318 comments in RQ1, we found that the manual assessment took 7.42 person hours on average.
However, this time may be inaccurate since practitioners did not make assessments under a controlled environment.

% nitty-gritty details
To be more precise, we considered only the time interval between two votes that are not more than 10 minutes apart.
The average time interval between votes is 28 seconds.
Thus, for 318 comments, the assessment time should be 7.42 person hours.
Using the same average interval time for all 10,583 comments,
we can imply that the manual assessment time would be 82.3 person hours.

% how our model helps
When applying our approach to classify these all comments, the models left only 23.5\% of comments for manual assessment (undetermined comments) as shown in Table \ref{tb:percent}.
This means that only 19.34 person hours of additional manual assessment of these undermined comments are needed, thus saving 76.5\% of human effort.

%To answer this question, timestamps are recorded as three people voted on each comment.
%Since three people work in their leisure time, there is much variability in the time between each vote.
%Therefore, we ignored the time interval between two votes if they are more than 10 minutes apart.
%
%On average, it takes 28 seconds to assess each comment.
%However, the median is 10 seconds, suggesting that most comments take only a little amount of time to assess, while only few comments take long time.
%This also suggests that the time to assess each comment is not normally distributed.
%The standard deviation is 45 seconds.
%
%To estimate the total time for us to assess these 318 comments,
%we multiplied the average time by the number of comments and the number of people.
%This results in 7.42 person hours.
%
%However, to manually assess all 10,583 comments---even with only one person---this would take 82.3 person hours.
%By running our model through all 10,583 comments, only 2,487 comments receive undetermined result.
%That means that about 77\% of the effort is saved.

\textbf{Assurance:} In Table \ref{tb:classify_number}, there are 88\% of useful class have comments with \texttt{YES} = 2 and 3 voting scores. The classification of useless class also have similar proportion i.e. having high number of comment with \texttt{YES} = 0 and 1 voting scores. We conjecture that the unclear comments with \texttt{YES} = 2 might actually be useful comments and comments with \texttt{YES} = 1 might actually be useless. The unclear result could be from error of manual assessment caused by human.
To verify this conjecture, we asked practitioners to re-examine the unclear comments.
We found that 17\% of these 66 comments became unclear because of human error.
This shows that some unclear comments are actually useful or useless, thus giving our model more confidence.
%This shows that our model \thai{what can we say about this model?}

%As the manual classification can be subjective, these comments were not received three voting scores.  



\textbf{Scalability:} Applying our classification model to all comments results in the proportions shown in Table \ref{tb:percent}.
As the table shows, the proportions are very similar in both the training dataset and all dataset. Indeed experiments for our case study using increasingly large random subsets of comments show the percentage of undetermined and overlap classification comments (and hence the manual effort required) is constant at about 23\%. While the percentage might vary for other projects, this result suggests that our method is scalable to very large numbers of comments. That is, the percentage of unclassified comments does not increase and hence we expect the method will reduce a substantial amount of manual assessment effort. This enables us to assess MCR comments for very large projects that currently are impractical to do without assistance. 
\dan{can we find a suitable example project to get some numbers for the example below?}
For example, OSS project XXX currently has YYY comments. Assuming constant effort and error rate (a dubious assumption), manual assessment effort for these would be estimated at ZZZ hours which is fundamentally impractical. However using similarity assessment with an estimated 23\% unclassified comments would reduce this effort to QQQ, a much more tractable effort.  

\begin{table}[!t]
\caption{Percentage of classifications results for all comments in the \texttt{qtbase} project }
\small
\centering
\def\arraystretch{1.2}
\begin{tabular}{c|c|c}
\hline

Result from & Sample Comments   &  All Comments \\ \cline{2-3}
Classification Models &  318 comments  & 10,583 comments \\ \hline \hline
Useful Class  & 41.8\% & 42.8\% \\
Useless Class   & 33.1\%  & 33.7\% \\
undetermined  & 25.1\% & 23.5\% \\ \hline
\end{tabular}
\label{tb:percent}
\end{table}

%\pick{Note from Dan sensei,}
%For cpost effectivness we can state the reduction percentage in the number of comments that much be checked manually.
% Also point out how much effort manual classification takes and that it is error prone (probably mor so than the automated method!
%You cannot, but some estimates are possble
%first, human beings have a fairly well known rate of making errors in classification
%second, look at the comments in the training set that the method classified differently, especially the unclear.
%Review these to see if the method suggested a better classification than was originally made by the assessors
%You can consider these errors. Now look at what wans the percent of errors in the training set
%This is also an estimate of the human error rate



