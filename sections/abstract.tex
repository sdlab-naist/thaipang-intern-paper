% !TEX root = ../paper.tex

\begin{abstract}
Modern Code Review (MCR) is an informal practice whereby reviewers virtually discuss proposed changes by adding comments through a code review tool or mailing list. It has received much research attention due to its perceived cost-effectiveness and popularity with industrial and OSS projects.
Recent studies indicate there is a positive relationship between the number of review comments and code quality.
However, little research exists investigating how such discussion impacts software quality.
The concern is that the informality of MCR encourages a focus on trivial, tangential, or unrelated issues.
Indeed, we have observed that such comments are quite frequent and may even constitute the majority.
We conjecture that an effective MCR actually depends on having a substantive quantity of comments that directly impact a proposed change (or are ``useful'').
To investigate this, a necessary first step requires distinguishing review comments that are useful to a proposed change from those that are not.
For a large OSS projects such as our Qt case study, manual assessment of the over 72,000 comments is a daunting task.
We propose to utilize semantic similarity as a practical, cost-efficient, and empirically assurable approach for assisting with the manual usefulness assessment of MCR comments.
Our case-study results indicate that our approach can classify comments with an average F-measure score of 0.73 and reduce comment usefulness assessment effort by about 77\%.


\begin{IEEEkeywords}
Modern Code Review, Software Quality, Text Mining
\end{IEEEkeywords}
\end{abstract}