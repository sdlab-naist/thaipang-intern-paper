\documentclass[conference]{IEEEtran}
\usepackage{microtype,mathtools,amsmath,multibib,amsfonts,multicol,array,multirow,place ins,cite,makecell,algorithm,algorithmic,subfig,paralist,graphicx}
%\usepackage{flushend}

\usepackage[font=small,labelfont=bf]{caption}

\usepackage{xspace}
\usepackage{color}
\usepackage{ifthen}
\usepackage{url}
\usepackage{fancybox}


\graphicspath{{zu/}}
\DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
\title{Title}

\author{
\IEEEauthorblockN{Thai Pangsakulyanont\IEEEauthorrefmark{1}, Patanamon Thongtanunam\IEEEauthorrefmark{2},  Daniel Port\IEEEauthorrefmark{3}, Hajimu Iida\IEEEauthorrefmark{2}}
\\
\IEEEauthorblockA{
\IEEEauthorrefmark{1} Kasetsart University, Thailand\\
@ku.ac.th
}

\IEEEauthorblockA{
\IEEEauthorrefmark{2} Nara Institute of Science and Technology, Japan\\
patanamon-t@is.naist.jp, iida@itc.naist.jp\\
}

\IEEEauthorblockA{
\IEEEauthorrefmark{3} University of Hawaii at Manoa, USA\\
dport@hawaii.edu \\
}
}


\maketitle
\newboolean{showcomments}
\setboolean{showcomments}{true} % toggle to show or hide comments
\ifthenelse{\boolean{showcomments}}
{\newcommand{\nbnote}[2]{
  % \fbox{\bfseries\sffamily\scriptsize#1}
  \fcolorbox{blue}{yellow}{\bfseries\sffamily\scriptsize#1}
  {\sf\small\textit{#2}}
  % \marginpar{\fbox{\bfseries\sffamily#1}}
 }
}
{\newcommand{\nbnote}[2]{}
 \newcommand{\version}{}
}
\newcommand\pick[1]{\nbnote{Pick sez}{\textcolor{magenta}{#1}}}
\newcommand\thai[1]{\nbnote{Thai sez}{\textcolor{blue}{#1}}}


\begin{abstract}

% the freeform style of modern code review

% some comments are not technically contributing


\begin{IEEEkeywords}
Modern Code Review, Software Quality, Text Mining
\end{IEEEkeywords}
\end{abstract}

\section{Introduction}
Software code review is an established method regarded as a best practice in Software Engineering to achieve a sufficient quality in software project. This method mainly intend to early identify defect before integrating changes. Nowadays, Modern Code Review (MCR)\cite{Bacchelli2013a}, less formal and lightweight code reviews have received much attention and  put into software development regularly in both industrial and OSS projects. A main benefit of MCR is the in-person meeting is not required as the formal inspection \cite{Fagan:1976:DCI:1661010.1661012}. Reviewers can discuss to find defects by comments through code review tool or mailing list. Then, developers will fix their changes following those comments.

As the performance of code review straight forward to the quality of software project, many studies investigated the influencing factors in MCR\cite{Baysal2001,Mcintosh,Beller,Hamasaki2013}. However, very little research investigate that in MCR, how does a discussion for a proposed change impact to the software quality. Since most of the proposed change improvements are triggered from reviewer comments \cite{Beller}. 
It is possible that comments of reviewers can be either positively contribute the proposed changes or a discussion which is out of scope.
To determine the impact of discussion in MCR, a manual classification is required as study of Beller et. al. \cite{Beller}. As a massive amount of changes and comments in MCR history\cite{Balachandran2013,Thongtanunam2014}, it is painstaking and time consuming to classify data sets to perform quantitative analysis. Moreover, the comments in MCR is unstructured natural text unlike a checklist in the formal inspection. 
%\pick{Need some connection. Says that Massive amount of commits and comment it would be expensive to manually identify. It's natural language }

In this paper, we present an approach to automatically classify the usefulness of comments. We define that the usefulness is a comment that contribute to improve the proposed changes. In our approach, we analyze the similarity between commit message of the proposed changes and their comments. Our key idea is that useful comments are likely to contain similar topic as the proposed changes. To do so, we use text mining techniques, Vector Space Model (VSM) to calculate similarity and use euclidian distance to calculate dissimilarity. 
We create our predictive model by estimating a set of similarity and dissimilarity values that best discriminate between a useful and useless comments using the prediction evaluations: Precision, Recall and F-measure.

For our empirical study, we used a review history of Gerrit\footnote{https://code.google.com/p/gerrit/} system which is tool for supporting MCR process. Our case study project is Qt\footnote{http://qt-project.org/}, a open source project of a cross-platform application and UI framework supported by Digia corporation. To validate our approach, we manually classify the usefulness for 320 comments and measure its accuracy. We also used our predictive model for the all set of comments to preliminary estimate quality of reviews. According to this, we address the following two research questions:

\noindent \textbf{RQ1:} Can we identify useful and useless discussions in code review?\\
\noindent \textbf{RQ2:} Do code reviewers intensively discuss on the proposed changes?

\noindent The main contribution of this paper are:
\begin{itemize}
\item We propose an approach to mine natural text of comments in code reviews and classify their usefulness.
\item The experimental results show that our approach can classify comments with XX of F-measure.
\item We found that XX\% of comments for each review are classified as useful comments, while XX \% of comments are classified as useless comments.
\end{itemize} 
%Software Inspection is basically composed of a three-step procedure: preparation, inspection meeting, and repair.

%Motivation Example.\thai{Wow}


\section{Background}
In this section, we describe MCR process based on Gerrit system. Then, we describe the background behind our approach composed of text mining techniques using VSM and euclidian distance and prediction evaluation techniques. 
\subsection{Modern Code Review Process}
\begin{figure}[!t]
\centering
\includegraphics[scale=0.35, trim= 100 110 50 80, clip=true]{review_process}
\caption{An simplified version of MCR Process based on Gerrit system}
\label{fig:process}
\end{figure}
Figure \ref{fig:process} shows an overview of MCR process based on Gerrit system. The grey box shows the review process of the system which composed of four main steps. (1) an author creating a patch and submitting a set of new or modified files as a review request to Gerrit system. (2) reviewers examine the proposed code changes whether it contains defects or not. (3) reviewers will give comments where should the author improve the code change. The author will create a new change according to the comments then re-submit a new patch again. Then, reviewers will examine the new change. If there is still a needed improvement, reviewers will give comments to fix again. The steps (1)-(3) will be iterated until reviewers can determine that this changes can be merge to the project or should not be merge (reject the change). 

According to this process, reviewers' comment is the most important for the software quality. McIntosh et. al. \cite{Mcintosh} also found that components which were reviewed without discussion are likely to contain bugs. However, as tool supporting MCR allows reviewers freely write a message to the author, it is often seen that some comments are not clearly identify defects and ineffective. Microsoft developers reported that they only focus on minor logic errors rather than discussing deeper in design\cite{Bacchelli2013a}. Our observation in Qt project also correspond this finding. We found some comments is superficial as shown in Fig. \ref{fig:example}(a), some comments only discuss about Version Control System (e.g. Git) used in the project. 

\begin{figure}[!t]
\centering
\includegraphics[scale=0.4, trim= 100 250 0 0, clip=true]{comment_examples}
\caption{Examples of comment in code reviews of Qt project.}
\label{fig:example}
\end{figure}

\subsection{Text Mining Techniques}
\subsubsection{tf--idf}
\subsubsection{Similarity Measure}
\subsection{Evaluation Techniques}
\subsubsection{Precision and Recall}
\subsubsection{F-measure}



\section{Classification Method}

\begin{figure*}
\centering
\includegraphics[width=6in]{overview}
\caption{Overview of our classification method.}
\label{fig:overview}
\end{figure*}

% where we got the data
We used the review data sets of the Qt project collected by Hamasaki et al TODO(cite properly).
Only reviews in the master branch of \texttt{qtbase} project are considered,
as it is the most active branch.

% quick structure
Each change includes a \emph{commit message} which describes what is changed.
After a change is submitted to Gerrit, reviewers can give scores to it.
The scores will determine whether the change will be accepted or not.
Reviewers can also add comments to the change, which may optionally be added to a specific line of code inside a changed file.
Majority of the comments are automatically generated by Gerrit and do not contain any user-written text.

% overview
In our classification method,
all commit messages and comments that are human-written are converted into vectors using tf--idf algorithm.
Few comments are sampled and then classified manually for use in training and validation.
A model is created from these training data, from which useful comments can automatically be identified.
This process is illustrated in Fig.\ref{fig:overview}.

\subsection{Data Preparation}

The commit messages and all comment texts are converted into vectors as part of the preparation step.
Fig. \ref{fig:preprocess} illustrates the overview of this step.
We wrote scripts in Ruby language to perform these tasks.

\begin{figure}[h]
\centering
\includegraphics[width=3in]{preprocess}
\caption{The data preparation process.
Automatically generated texts are removed, remaining text is tokenized, stop words are removed, remaining tokens are stemmed,
and are converted into a vector.}
\label{fig:preprocess}
\end{figure}

\subsubsection{Removal of automatically generated text}

First, all comments by the Gerrit system, \emph{Qt Sanity Bot}, and \emph{Qt Continuous Integration System} are skipped.

% conversion into vector, wow
Next, we looked for common patterns that appeared in the comments, because it is very likely that they are automatically generated.
This is accomplished by splitting texts into lines, and for each line, searching for words that can be found in the English wordlist\footnote{The wordlist found in \texttt{/usr/share/dict/words} from Ubuntu Linux distribution is used.}.
This effectively removed the ID numbers and other non-generic terms.

Next, the lines that appears most frequently are identified.
From these lines, we then constructed the regular expression patterns,
and finally, these patterns are used to filter out automatically generated text from our data.

\subsubsection{Tokenization, stop word removal and stemming}

After the automatically generated messages are removed, we extracted the words in each document into a list of tokens by searching for alphanumeric characters including apostrophes.
Stop words from the Google stop word list\footnote{Available at \url{http://meta.wikimedia.org/wiki/Stop_word_list/google_stop_word_list#English}} are then removed.
The stemming is performed on remaining tokens using Porter stemming algorithm.
The remaining words are then combined to form a corpus of all used words.

\subsubsection{Conversion into vector}

Finally, the tf--idf algorithm is used to convert each document into a vector.


\subsection{Training Data}

We sampled 320 comments from the data set.
These samples are then classified as either useful (that is, technically contributing to the software) or not useful.
This task is carried out by three people who worked independently.

Each comment is then scored based on the number of positive answers.
For example, a comment with a score of 3 means that all three people said that it is useful,
while a comment with a score of 0 means that none of us marked it as useful.

%The score of a comment can be defined as:

%\begin{align*}
%Score_i & = \sum_{\text{reviewer } r} Review(r, i). \\
%Review(r, i) & = \begin{cases}
%	1 & \text{if } r \text{ says that the comment } i \text{ is useful,} \\
%	0 & \text{otherwise.}
%\end{cases}
%\end{align*}

\subsection{Model Generation and Validation}

% the metrics
For each comment, we computed similarity and dissimilarity metrics
between the comment text and the corresponding commit message.
We chose cosine similarity and euclidean distance as the metrics to use in model generation.

\begin{figure}[h]
\centering
\includegraphics[width=3in]{vector}
\caption{The similarity and distance metric that is being used.}
\label{fig:vector}
\end{figure}

% assumptions in categorization
The model is based on two assumptions: that useful comments will have a $similarity \geq A \text{ and } distance \leq B$,
and that comments that are not useful will have a $ similarity \leq C \text{ and } distance \geq D$,
where $A$, $B$, $C$, and $D$ are some constant.\footnote{We also tried using the `or' operator, and found that using `and' produces better result.}

% finding parameters
To find these parameters, a brute-force approach is used.
This is possible because the set of $similarity$ and $distance$ values are discrete.
Each possible value of $A$, $B$, $C$, and $D$ are evaluated to find the constant that returns the maximum F$_1$ score.

% validation
To validate our model, 10-fold cross validation is performed.
The values of precision, recall, F$_1$ score, and accuracy are recorded and then averaged
to give the overall performance of our model.


\section{Results}

\subsection{Data Preparation}

In total, there are 6,605 changes and 72,484 comments. 25,076 comments are by the system or one of the bots.

\subsubsection{Common pattern removal}

Many common patterns are found, such as \emph{`Uploaded patch set 2.'} and \emph{`Change has been successfully cherry-picked to the staging branch as \dots'}.
The occurrences of these patterns are removed from our data set.
This leaves 36,738 comments empty, which probably means that the whole comment is automatically generated.
Only 17,275 comments and commit messages are left.

\subsubsection{Tokenizing}

After preprocessing, 393,238 tokens are generated in total. They are composed of 20,025 different words.
This means that each document will be converted into a vector of 20,025 dimensions, each dimension representing a single word.



\subsection{Training Data}

After the 320 sample comments have been labeled,
they are scored based on the number of ``yes'' given to the sample.
There were 87, 60, 51, and 122 comments with scores of 0, 1, 2, and 3, respectively.

After the similarity and distance metrics have been calculated,
these metrics appear to be able to separate the useful comments from the non-useful ones,
as can be seen in Fig.\ref{fig:scatter}.
Note that many comments have a cosine similarity metric of 0.
This is because the comment text and the corresponding commit message has no word in comment.

\begin{figure}[h]
\centering
\includegraphics[width=3in]{scatter}
\caption{The similarity and distance plot of the training data.
The symbol represents the score, which ranges from 0 to 3.}
\label{fig:scatter}
\end{figure}



\subsection{Model Generation and Validation}

% the criteria that were found
From our training data,
the following criteria have been obtained that maximizes the F$_1$ score:

Criteria to find samples with score of 3 (``positive''):
\begin{gather*} similarity \geq 0.015528 \text{ and } distance \leq 2.494944
\\ \text{(F$_1$-score: 0.741313)} \end{gather*}

Criteria to find samples with score of 0 (``negative''):
\begin{gather*} similarity \leq 0.087522 \text{ and } distance \geq 2.265679
\\ \text{(F$_1$-score: 0.725389)}\end{gather*}

Note that these constant values can vary from project to project, and thus is not a universal constant.

% run result

Running our model against the training data gives us the result displayed in the following table.
In the table, \emph{Neither} means that the comment did not meet either criteria, while \emph{Overlap} means that the comment  met both ``positive'' and ``negative'' criteria.

\begin{center}
\begin{tabular}{|r|rrrr|}
\hline
& \bfseries 0 & \bfseries 1 & \bfseries 2 & \bfseries 3 \\
\hline
Negative & 69 & 23 & 7 & 6 \\
Neither & 12 & 24 & 20 & 20 \\
Overlap & 1 & 1 & 0 & 1 \\
Positive & 4 & 13 & 24 & 95 \\
\hline
\end{tabular}
\end{center}

As expected, most comments classified as \emph{negative} have score of 0 and 1,
while most \emph{positive} comments have score of 2 and 3.
However, some comments in every score were lost and classified as \emph{neither}.

% cross validation result


\section{Discussion}





\section{Threat to Validity}


\section{Conclusion and Future Work}


\IEEEpeerreviewmaketitle

\bibliographystyle{IEEEtran}

\bibliography{references}



% that's all folks
\end{document}


